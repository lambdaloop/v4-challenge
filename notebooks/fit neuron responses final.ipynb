{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4 Neural Data Challenge - Model Fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hspace{3mm}$To begin, we extracted several basic features which we thought could explain neural responses to some degree. We started by using the raw RGB values, the pixel values in LAB space (an alternate color space), the power spectrum of the 2D fourier transform, the components of a discrete wavelet transformation of the image (using a Gabor filter), and finally several basic image statistics. These statistics are calculated only on the center of the image. They are: the minimum pixel value, the maximum pixel value, the mean pixel value, and the standard deviation of the pixel values. These were computed for each channel (RGB). <br><br>\n",
    "$\\hspace{3mm}$ We also included the output of Alexnet as features to regress upon. Briefly, we normalized and rescaled the images, ran them through Alexnet, and got the response of all 5 convolutional layers. We only used the responses to the center of the image (e.g., conv1 is 55 x 55 x 196, we only use the middle 20 x 20 x 196 block of responses). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to compute the basic image features can be found at: <br>\n",
    "https://github.com/lambdaloop/v4-challenge/blob/master/compute_features.py\n",
    "\n",
    "The code to compute the Alexnet features can be found at: <br>\n",
    "https://github.com/lambdaloop/v4-challenge/blob/master/nn_output_aspredictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "from tqdm import trange, tqdm\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of machine learning stuff\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure styling\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', palette='colorblind')\n",
    "\n",
    "rcParams['figure.figsize'] = (13,4.5)\n",
    "rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('../data/stim.npy')\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/sub.csv')\n",
    "\n",
    "responses = np.array(train.iloc[:, 1:])\n",
    "\n",
    "dd = np.load('../data/features.npz')\n",
    "features = dict()\n",
    "for k in dd.keys():\n",
    "    features[k] = dd[k]\n",
    "\n",
    "    \n",
    "conv_train = np.load('../data/conv_train.npy').flat[0]\n",
    "conv_test = np.load('../data/conv_test.npy').flat[0]\n",
    "\n",
    "conv_features = dict()\n",
    "for k in conv_train.keys():\n",
    "    conv_features[k] = np.vstack([conv_test[k], conv_train[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['raw', 'LAB', 'edges', 'fourier', 'gabor', 'stats', 'stats_LAB', 'stats_HSV', 'stats_edges'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1', 'conv2', 'conv3', 'conv4', 'conv5'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and choosing features\n",
    "\n",
    "In the following section, we preprocess all the features in order to feed them our regressor.\n",
    "\n",
    "We found that any fitting algorithm performed poorly with too many predictors, so we projected each feature with too many predictors onto its principal components. \n",
    "\n",
    "In addition, we also suspected that the visual system may be responding to the position of images on a manifold. In order to get at this manifold, we also computed a projection using the [UMAP](https://github.com/lmcinnes/umap) embedding. \n",
    "\n",
    "The number of components for both PCA and UMAP was optimized manually to minimize the cross-validated mean rmse. It is likely that optimizing these parameters with a proper search could lead to a better solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_lab = PCA(n_components=50).fit_transform(features['LAB'])\n",
    "pcs_fourier = PCA(n_components=30).fit_transform(features['fourier'])\n",
    "pcs_gabor = PCA(n_components=20).fit_transform(features['gabor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_raw = PCA(n_components=20).fit_transform(features['raw'])\n",
    "pcs_edges = PCA(n_components=20).fit_transform(features['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_conv1 = PCA(n_components=30).fit_transform(conv_features['conv1'])\n",
    "pcs_conv2 = PCA(n_components=20).fit_transform(conv_features['conv2'])\n",
    "pcs_conv3 = PCA(n_components=20).fit_transform(conv_features['conv3'])\n",
    "pcs_conv4 = PCA(n_components=20).fit_transform(conv_features['conv4'])\n",
    "pcs_conv5 = PCA(n_components=20).fit_transform(conv_features['conv5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(metric='correlation', min_dist=1.0, n_components=100)\n",
    "X_embed = embedding.fit_transform(features['raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(metric='correlation', min_dist=1.0, n_components=10)\n",
    "X_embed_edges = embedding.fit_transform(features['edges'])\n",
    "\n",
    "embedding = umap.UMAP(metric='correlation', min_dist=1.0, n_components=10)\n",
    "X_embed_fourier = embedding.fit_transform(features['fourier'])\n",
    "\n",
    "embedding = umap.UMAP(metric='correlation', min_dist=1.0, n_components=10)\n",
    "X_embed_lab = embedding.fit_transform(features['LAB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 470)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = np.hstack([\n",
    "   X_embed,  \n",
    "   X_embed_edges,\n",
    "   X_embed_lab, \n",
    "   X_embed_fourier,\n",
    "   features['stats'], \n",
    "   features['stats_LAB'], \n",
    "   features['stats_edges'],\n",
    "   pcs_lab,\n",
    "   pcs_fourier, \n",
    "   pcs_gabor,\n",
    "   pcs_edges,\n",
    "   pcs_raw,\n",
    "   pcs_conv1, \n",
    "   pcs_conv2, \n",
    "   pcs_conv3,\n",
    "   pcs_conv4,  \n",
    "   pcs_conv5,\n",
    "                  ])\n",
    "\n",
    "X_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and evaluating the model\n",
    "\n",
    "Next, we take all the features and fit the model neuron by neuron. \n",
    "We played around with a couple models in scikit-learn, and found the extra-trees regressor to perform the best. This estimator averages a bunch of decision trees fit on the sub-samples of the data. This allows the model to fit nonlinearities and interactions but controlling for overfitting. \n",
    "\n",
    "Here, we fit and evaluate an extra-trees regressor with some simpler parameters to check our cross-validation score quickly. We print out the mean rmse with and without neuron 17. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9401612957630908\n",
      "1 0.6672550582675812\n",
      "2 0.6425172425653005\n",
      "3 0.26917291291149953\n",
      "4 0.7629908346594553\n",
      "5 1.039534896806068\n",
      "6 1.0740585615427993\n",
      "7 0.4782933509971528\n",
      "8 0.5247241922388973\n",
      "9 0.5402333861880092\n",
      "10 0.29454477061921747\n",
      "11 0.8026178731737081\n",
      "12 0.6909614742141151\n",
      "13 0.5660701433568197\n",
      "14 0.7181016896970075\n",
      "15 0.5727938179694713\n",
      "16 1.1006788621546244\n",
      "17 0.4338199426080432\n",
      "mean rmse: 0.6732516836518256\n",
      "mean rmse without 17: 0.6873359037132246\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesRegressor(max_depth=None, max_features='sqrt', criterion=\"mse\", n_estimators=100, n_jobs=1)\n",
    "\n",
    "all_scores = []\n",
    "for i in range(responses.shape[1]):\n",
    "    vals = responses[:, i]\n",
    "    good = ~np.isnan(vals)\n",
    "    scores = cross_val_score(model, X_all[50:][good], vals[good], \n",
    "                             scoring=make_scorer(mean_squared_error), cv=3)\n",
    "    scores = np.sqrt(scores)\n",
    "    print(i, np.mean(scores))\n",
    "    all_scores.append(np.mean(scores))\n",
    "    \n",
    "print('mean rmse:', np.mean(all_scores))\n",
    "print('mean rmse without 17:', np.mean(all_scores[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining predictions\n",
    "\n",
    "Finally, we use the same model but with more estimators as the basis for our final prediction. We generate two sets of predictions. The first consists of the predictions of extra-trees model fit on each neuron. The second has all the same predictions, but with a prediction of the mean for neuron 17. \n",
    "\n",
    "From our tests, we found that for any model or feature set, we always obtained a negative R^2 value for neuron 17 predictions, which means it is better to predict the mean response than whatever our model outputs. We suspect that neuron 17 does not respond to any visual features consistently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:17<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesRegressor(max_depth=None, n_estimators=400, n_jobs=4)\n",
    "\n",
    "new_test = test.copy()\n",
    "\n",
    "for i in trange(responses.shape[1]):\n",
    "    vals = responses[:, i]\n",
    "    good = ~np.isnan(vals)\n",
    "    model.fit(X_all[50:][good], vals[good])\n",
    "    pred = model.predict(X_all[:50])\n",
    "    new_test.iloc[:, i+1] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.to_csv('../data/output_umap_nnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_17 = new_test.copy()\n",
    "new_test_17.iloc[:, 18] = np.mean(responses[:, 17])\n",
    "new_test_17.to_csv('../data/output_umap_nnet_blank17.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we tried but didn't pan out\n",
    "\n",
    "Finally, in the interest of science, we report on the things we tried but couldn't get to work as well as our current solution\n",
    "\n",
    "- Using features from VGG19 instead of AlexNet\n",
    "- Predicting on features directly instead of PCs of features (too slow)\n",
    "- Using ridge or lasso regression over any set of features (including VGG or AlexNet)\n",
    "- Using random forest model instead of extra-trees model (slower and worse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
