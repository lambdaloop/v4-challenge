{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Neural Network Features for UWNDC2019 Model \n",
    "\n",
    "<br><br>\n",
    "### Uses Alexnet and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from collections import namedtuple\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore',category=FutureWarning)\n",
    "\n",
    "datadir ='./Hackathon/v4-challenge/data/' # set this to the location of your data directory which\n",
    "                   # should contain train.csv and stim.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_now = pd.read_csv(datadir+'train.csv')\n",
    "im_now = np.load(datadir+'stim.npy')\n",
    "\n",
    "test_ims = im_now[50:,...]\n",
    "train_ims = im_now[50:,...]\n",
    "\n",
    "#calculate mean for image normalization\n",
    "im_mean = np.mean(np.mean(im_now,axis=(1,2)),axis=0)\n",
    "im_std = np.std(np.std(im_now,axis=(1,2)),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v4_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df,ims, transform):\n",
    "        \n",
    "        self.resp_frame = df\n",
    "        self.transform = transform\n",
    "        self.ims = ims\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.resp_frame)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img = Image.fromarray(np.uint8(self.ims[idx,:,:,:]*255))\n",
    "        responses = self.resp_frame.iloc[idx,1:].as_matrix()\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        sample = {'image': img, 'responses': responses}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "tfm = transforms.Compose([transforms.Resize(224),transforms.ToTensor(), transforms.Normalize(mean=im_mean,std=im_std)])\n",
    "\n",
    "train_set = v4_dataset(df_now,train_ims,tfm)\n",
    "trainloader = DataLoader(train_set,batch_size=4)\n",
    "\n",
    "test_set = v4_dataset(df_now,test_ims,tfm)\n",
    "testloader = DataLoader(test_set,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7246376811594203% done with train examples\n",
      "1.4492753623188406% done with train examples\n",
      "2.1739130434782608% done with train examples\n",
      "2.898550724637681% done with train examples\n",
      "3.6231884057971016% done with train examples\n",
      "4.3478260869565215% done with train examples\n",
      "5.072463768115942% done with train examples\n",
      "5.797101449275362% done with train examples\n",
      "6.521739130434782% done with train examples\n",
      "7.246376811594203% done with train examples\n",
      "7.971014492753623% done with train examples\n",
      "8.695652173913043% done with train examples\n",
      "9.420289855072463% done with train examples\n",
      "10.144927536231885% done with train examples\n",
      "10.869565217391305% done with train examples\n",
      "11.594202898550725% done with train examples\n",
      "12.318840579710145% done with train examples\n",
      "13.043478260869565% done with train examples\n",
      "13.768115942028986% done with train examples\n",
      "14.492753623188406% done with train examples\n",
      "15.217391304347826% done with train examples\n",
      "15.942028985507246% done with train examples\n",
      "16.666666666666668% done with train examples\n",
      "17.391304347826086% done with train examples\n",
      "18.115942028985508% done with train examples\n",
      "18.840579710144926% done with train examples\n",
      "19.565217391304348% done with train examples\n",
      "20.28985507246377% done with train examples\n",
      "21.014492753623188% done with train examples\n",
      "21.73913043478261% done with train examples\n",
      "22.463768115942027% done with train examples\n",
      "23.18840579710145% done with train examples\n",
      "23.91304347826087% done with train examples\n",
      "24.63768115942029% done with train examples\n",
      "25.36231884057971% done with train examples\n",
      "26.08695652173913% done with train examples\n",
      "26.81159420289855% done with train examples\n",
      "27.536231884057973% done with train examples\n",
      "28.26086956521739% done with train examples\n",
      "28.985507246376812% done with train examples\n",
      "29.71014492753623% done with train examples\n",
      "30.434782608695652% done with train examples\n",
      "31.159420289855074% done with train examples\n",
      "31.884057971014492% done with train examples\n",
      "32.608695652173914% done with train examples\n",
      "33.333333333333336% done with train examples\n",
      "34.05797101449275% done with train examples\n",
      "34.78260869565217% done with train examples\n",
      "35.507246376811594% done with train examples\n",
      "36.231884057971016% done with train examples\n",
      "36.95652173913044% done with train examples\n",
      "37.68115942028985% done with train examples\n",
      "38.405797101449274% done with train examples\n",
      "39.130434782608695% done with train examples\n",
      "39.85507246376812% done with train examples\n",
      "40.57971014492754% done with train examples\n",
      "41.30434782608695% done with train examples\n",
      "42.028985507246375% done with train examples\n",
      "42.7536231884058% done with train examples\n",
      "43.47826086956522% done with train examples\n",
      "44.20289855072464% done with train examples\n",
      "44.927536231884055% done with train examples\n",
      "45.65217391304348% done with train examples\n",
      "46.3768115942029% done with train examples\n",
      "47.10144927536232% done with train examples\n",
      "47.82608695652174% done with train examples\n",
      "48.55072463768116% done with train examples\n",
      "49.27536231884058% done with train examples\n",
      "50.0% done with train examples\n",
      "50.72463768115942% done with train examples\n",
      "51.44927536231884% done with train examples\n",
      "52.17391304347826% done with train examples\n",
      "52.89855072463768% done with train examples\n",
      "53.6231884057971% done with train examples\n",
      "54.34782608695652% done with train examples\n",
      "55.072463768115945% done with train examples\n",
      "55.79710144927536% done with train examples\n",
      "56.52173913043478% done with train examples\n",
      "57.2463768115942% done with train examples\n",
      "57.971014492753625% done with train examples\n",
      "58.69565217391305% done with train examples\n",
      "59.42028985507246% done with train examples\n",
      "60.14492753623188% done with train examples\n",
      "60.869565217391305% done with train examples\n",
      "61.594202898550726% done with train examples\n",
      "62.31884057971015% done with train examples\n",
      "63.04347826086956% done with train examples\n",
      "63.768115942028984% done with train examples\n",
      "64.4927536231884% done with train examples\n",
      "65.21739130434783% done with train examples\n",
      "65.94202898550725% done with train examples\n",
      "66.66666666666667% done with train examples\n",
      "67.3913043478261% done with train examples\n",
      "68.1159420289855% done with train examples\n",
      "68.84057971014492% done with train examples\n",
      "69.56521739130434% done with train examples\n",
      "70.28985507246377% done with train examples\n",
      "71.01449275362319% done with train examples\n",
      "71.73913043478261% done with train examples\n",
      "72.46376811594203% done with train examples\n",
      "73.18840579710145% done with train examples\n",
      "73.91304347826087% done with train examples\n",
      "74.6376811594203% done with train examples\n",
      "75.3623188405797% done with train examples\n",
      "76.08695652173913% done with train examples\n",
      "76.81159420289855% done with train examples\n",
      "77.53623188405797% done with train examples\n",
      "78.26086956521739% done with train examples\n",
      "78.98550724637681% done with train examples\n",
      "79.71014492753623% done with train examples\n",
      "80.43478260869566% done with train examples\n",
      "81.15942028985508% done with train examples\n",
      "81.8840579710145% done with train examples\n",
      "82.6086956521739% done with train examples\n",
      "83.33333333333333% done with train examples\n",
      "84.05797101449275% done with train examples\n",
      "84.78260869565217% done with train examples\n",
      "85.5072463768116% done with train examples\n",
      "86.23188405797102% done with train examples\n",
      "86.95652173913044% done with train examples\n",
      "87.68115942028986% done with train examples\n",
      "88.40579710144928% done with train examples\n",
      "89.1304347826087% done with train examples\n",
      "89.85507246376811% done with train examples\n",
      "90.57971014492753% done with train examples\n",
      "91.30434782608695% done with train examples\n",
      "92.02898550724638% done with train examples\n",
      "92.7536231884058% done with train examples\n",
      "93.47826086956522% done with train examples\n",
      "94.20289855072464% done with train examples\n",
      "94.92753623188406% done with train examples\n",
      "95.65217391304348% done with train examples\n",
      "96.3768115942029% done with train examples\n",
      "97.10144927536231% done with train examples\n",
      "97.82608695652173% done with train examples\n",
      "98.55072463768116% done with train examples\n",
      "99.27536231884058% done with train examples\n",
      "100.0% done with train examples\n",
      "0.7246376811594203% done with test examples\n",
      "1.4492753623188406% done with test examples\n",
      "2.1739130434782608% done with test examples\n",
      "2.898550724637681% done with test examples\n",
      "3.6231884057971016% done with test examples\n",
      "4.3478260869565215% done with test examples\n",
      "5.072463768115942% done with test examples\n",
      "5.797101449275362% done with test examples\n",
      "6.521739130434782% done with test examples\n",
      "7.246376811594203% done with test examples\n",
      "7.971014492753623% done with test examples\n",
      "8.695652173913043% done with test examples\n",
      "9.420289855072463% done with test examples\n",
      "10.144927536231885% done with test examples\n",
      "10.869565217391305% done with test examples\n",
      "11.594202898550725% done with test examples\n",
      "12.318840579710145% done with test examples\n",
      "13.043478260869565% done with test examples\n",
      "13.768115942028986% done with test examples\n",
      "14.492753623188406% done with test examples\n",
      "15.217391304347826% done with test examples\n",
      "15.942028985507246% done with test examples\n",
      "16.666666666666668% done with test examples\n",
      "17.391304347826086% done with test examples\n",
      "18.115942028985508% done with test examples\n",
      "18.840579710144926% done with test examples\n",
      "19.565217391304348% done with test examples\n",
      "20.28985507246377% done with test examples\n",
      "21.014492753623188% done with test examples\n",
      "21.73913043478261% done with test examples\n",
      "22.463768115942027% done with test examples\n",
      "23.18840579710145% done with test examples\n",
      "23.91304347826087% done with test examples\n",
      "24.63768115942029% done with test examples\n",
      "25.36231884057971% done with test examples\n",
      "26.08695652173913% done with test examples\n",
      "26.81159420289855% done with test examples\n",
      "27.536231884057973% done with test examples\n",
      "28.26086956521739% done with test examples\n",
      "28.985507246376812% done with test examples\n",
      "29.71014492753623% done with test examples\n",
      "30.434782608695652% done with test examples\n",
      "31.159420289855074% done with test examples\n",
      "31.884057971014492% done with test examples\n",
      "32.608695652173914% done with test examples\n",
      "33.333333333333336% done with test examples\n",
      "34.05797101449275% done with test examples\n",
      "34.78260869565217% done with test examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.507246376811594% done with test examples\n",
      "36.231884057971016% done with test examples\n",
      "36.95652173913044% done with test examples\n",
      "37.68115942028985% done with test examples\n",
      "38.405797101449274% done with test examples\n",
      "39.130434782608695% done with test examples\n",
      "39.85507246376812% done with test examples\n",
      "40.57971014492754% done with test examples\n",
      "41.30434782608695% done with test examples\n",
      "42.028985507246375% done with test examples\n",
      "42.7536231884058% done with test examples\n",
      "43.47826086956522% done with test examples\n",
      "44.20289855072464% done with test examples\n",
      "44.927536231884055% done with test examples\n",
      "45.65217391304348% done with test examples\n",
      "46.3768115942029% done with test examples\n",
      "47.10144927536232% done with test examples\n",
      "47.82608695652174% done with test examples\n",
      "48.55072463768116% done with test examples\n",
      "49.27536231884058% done with test examples\n",
      "50.0% done with test examples\n",
      "50.72463768115942% done with test examples\n",
      "51.44927536231884% done with test examples\n",
      "52.17391304347826% done with test examples\n",
      "52.89855072463768% done with test examples\n",
      "53.6231884057971% done with test examples\n",
      "54.34782608695652% done with test examples\n",
      "55.072463768115945% done with test examples\n",
      "55.79710144927536% done with test examples\n",
      "56.52173913043478% done with test examples\n",
      "57.2463768115942% done with test examples\n",
      "57.971014492753625% done with test examples\n",
      "58.69565217391305% done with test examples\n",
      "59.42028985507246% done with test examples\n",
      "60.14492753623188% done with test examples\n",
      "60.869565217391305% done with test examples\n",
      "61.594202898550726% done with test examples\n",
      "62.31884057971015% done with test examples\n",
      "63.04347826086956% done with test examples\n",
      "63.768115942028984% done with test examples\n",
      "64.4927536231884% done with test examples\n",
      "65.21739130434783% done with test examples\n",
      "65.94202898550725% done with test examples\n",
      "66.66666666666667% done with test examples\n",
      "67.3913043478261% done with test examples\n",
      "68.1159420289855% done with test examples\n",
      "68.84057971014492% done with test examples\n",
      "69.56521739130434% done with test examples\n",
      "70.28985507246377% done with test examples\n",
      "71.01449275362319% done with test examples\n",
      "71.73913043478261% done with test examples\n",
      "72.46376811594203% done with test examples\n",
      "73.18840579710145% done with test examples\n",
      "73.91304347826087% done with test examples\n",
      "74.6376811594203% done with test examples\n",
      "75.3623188405797% done with test examples\n",
      "76.08695652173913% done with test examples\n",
      "76.81159420289855% done with test examples\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "        features = list(models.alexnet(pretrained=True).features)\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        results =[]\n",
    "        for ii, cv_layer in enumerate(self.features):\n",
    "            x = cv_layer(x)\n",
    "            if ii in {0, 3, 6, 8, 10}: #just conv1-5\n",
    "                results.append(x)\n",
    "        \n",
    "        an_outputs = namedtuple(\"AlexNetOutputs\",['conv1','conv2','conv3','conv4','conv5'])\n",
    "        return an_outputs(*results)\n",
    "    \n",
    "net = AlexNet()\n",
    "net.eval()\n",
    "\n",
    "conv_train = dict()\n",
    "conv_test = dict()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "for i, data in enumerate(trainloader):\n",
    "    \n",
    "    inputs = data['image']\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    if i == 0:\n",
    "        for j, fld in enumerate(outputs._fields):\n",
    "            #import pdb; pdb.set_trace()\n",
    "            conv_train[fld] = outputs[j].cpu().detach().numpy()\n",
    "    else:\n",
    "        for j, fld in enumerate(outputs._fields):\n",
    "            conv_train[fld] = np.vstack((conv_train[fld],outputs[j].cpu().detach().numpy()))\n",
    "   \n",
    "    print('{}% done with train examples'.format(round((i+1)*100/len(trainloader),2)))\n",
    "\n",
    "\n",
    "for i, data in enumerate(testloader):\n",
    "    \n",
    "    inputs = data['image']\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    if i == 0:\n",
    "        for j, fld in enumerate(outputs._fields):\n",
    "            #import pdb; pdb.set_trace()\n",
    "            conv_test[fld] = outputs[j].cpu().detach().numpy()\n",
    "    else:\n",
    "        for j, fld in enumerate(outputs._fields):\n",
    "            conv_test[fld] = np.vstack((conv_test[fld],outputs[j].cpu().detach().numpy()))\n",
    "            \n",
    "    print('{}% done with test examples'.format(round((i+1)*100/len(testloader),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
